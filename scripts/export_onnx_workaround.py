#!/usr/bin/env python3
"""
ONNX Export with Attention Workaround

Disables scaled_dot_product_attention to allow ONNX export.
"""

import os
import sys
from pathlib import Path
import torch

def export_onnx_with_workaround():
    print("=" * 70)
    print("SafeSpark - ONNX Export with Attention Workaround")
    print("=" * 70)
    
    # Output path
    output_dir = Path(__file__).parent.parent / "app" / "src" / "main" / "assets"
    output_file = output_dir / "minilm_encoder.onnx"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nüìÅ Output: {output_file}")
    
    try:
        from sentence_transformers import SentenceTransformer
        import onnx
        
        print("\nüîÑ Loading model...")
        model = SentenceTransformer(
            "paraphrase-multilingual-MiniLM-L12-v2",
            device='cpu'
        )
        
        transformer = model[0].auto_model
        transformer = transformer.to('cpu')
        transformer.eval()
        
        # WORKAROUND: Disable scaled_dot_product_attention
        print("   üîß Disabling scaled_dot_product_attention...")
        
        # Set use_sdpa to False to avoid the problematic attention
        for module in transformer.modules():
            if hasattr(module, '_use_sdpa'):
                module._use_sdpa = False
            if hasattr(module, 'use_sdpa'):
                module.use_sdpa = False
        
        # Alternative: Patch the config
        if hasattr(transformer, 'config'):
            if hasattr(transformer.config, '_attn_implementation'):
                transformer.config._attn_implementation = 'eager'
            if hasattr(transformer.config, 'use_sdpa'):
                transformer.config.use_sdpa = False
        
        print("   ‚úÖ Model loaded with workaround")
        
        # Create dummy input
        tokenizer = model.tokenizer
        dummy_text = "Test sentence for ONNX export."
        inputs = tokenizer(
            dummy_text,
            return_tensors="pt",
            padding="max_length",
            max_length=128,
            truncation=True
        )
        
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        
        print(f"   Input shape: {input_ids.shape}")
        
        # Export to ONNX with lower opset
        print("\n   üîÑ Exporting to ONNX (opset 12 for compatibility)...")
        
        temp_onnx = "/tmp/minilm_safespark.onnx"
        
        with torch.no_grad():
            # Use torch.jit.trace instead of torch.onnx.export
            print("   Using TorchScript trace method...")
            
            # Create a wrapper that returns only what we need
            class ModelWrapper(torch.nn.Module):
                def __init__(self, model):
                    super().__init__()
                    self.model = model
                    
                def forward(self, input_ids, attention_mask):
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask
                    )
                    # Return only last_hidden_state
                    return outputs.last_hidden_state
            
            wrapped = ModelWrapper(transformer)
            wrapped.eval()
            
            # Trace the model
            traced = torch.jit.trace(
                wrapped,
                (input_ids, attention_mask),
                strict=False
            )
            
            print("   ‚úÖ Model traced successfully")
            
            # Now export traced model to ONNX
            torch.onnx.export(
                traced,
                (input_ids, attention_mask),
                temp_onnx,
                input_names=["input_ids", "attention_mask"],
                output_names=["last_hidden_state"],
                dynamic_axes={
                    "input_ids": {0: "batch_size", 1: "sequence_length"},
                    "attention_mask": {0: "batch_size", 1: "sequence_length"},
                    "last_hidden_state": {0: "batch_size", 1: "sequence_length"}
                },
                opset_version=12,  # Lower opset for better compatibility
                do_constant_folding=True,
                verbose=False
            )
        
        print("   ‚úÖ ONNX export successful")
        
        # Verify ONNX
        print("\n   üîç Verifying ONNX model...")
        onnx_model = onnx.load(temp_onnx)
        onnx.checker.check_model(onnx_model)
        print("   ‚úÖ ONNX model is valid")
        
        # Move to final location
        import shutil
        shutil.move(temp_onnx, str(output_file))
        
        size_mb = output_file.stat().st_size / (1024 * 1024)
        print(f"\nüì¶ Final size: {size_mb:.2f} MB")
        
        # Test with ONNX Runtime
        print("\n   üß™ Testing ONNX Runtime...")
        import onnxruntime as ort
        
        session = ort.InferenceSession(
            str(output_file),
            providers=['CPUExecutionProvider']
        )
        
        print(f"   Providers: {session.get_providers()}")
        
        # Run inference test
        outputs = session.run(
            None,
            {
                "input_ids": input_ids.numpy(),
                "attention_mask": attention_mask.numpy()
            }
        )
        
        print(f"   Output shape: {outputs[0].shape}")
        
        # Test mean pooling
        import numpy as np
        hidden_states = outputs[0]
        mask_expanded = np.expand_dims(attention_mask.numpy(), -1)
        sum_embeddings = np.sum(hidden_states * mask_expanded, axis=1)
        sum_mask = np.clip(np.sum(mask_expanded, axis=1), a_min=1e-9, a_max=None)
        embeddings = sum_embeddings / sum_mask
        
        print(f"   Final embedding shape: {embeddings.shape}")
        print(f"   Embedding dimension: {embeddings.shape[1]}")
        
        if embeddings.shape[1] == 384:
            print("   ‚úÖ Correct embedding dimension (384)!")
        else:
            print(f"   ‚ö†Ô∏è Unexpected dimension: {embeddings.shape[1]}")
        
        print("   ‚úÖ ONNX Runtime works perfectly!")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("\nüöÄ Starting ONNX export with workaround...")
    
    success = export_onnx_with_workaround()
    
    if success:
        print("\n" + "=" * 70)
        print("‚úÖ SUCCESS! ONNX model is ready for SafeSpark!")
        print("=" * 70)
        print("\nüìã Next steps:")
        print("1. cd /Users/knutludtmann/AndroidStudioProjects/SafeSpark")
        print("2. ./gradlew clean assembleDebug")
        print("3. ./gradlew connectedDebugAndroidTest")
        print("\nüéØ All 45 tests should pass now!")
        print("üéØ Production-ready with semantic understanding!")
        sys.exit(0)
    else:
        print("\n" + "=" * 70)
        print("‚ùå FAILED! Could not export ONNX model")
        print("=" * 70)
        sys.exit(1)
